# ç¬¬äºŒå‘¨: å·¥ä½œæµç¼–æŽ’

è§†é¢‘èµ„æºåœ°å€ï¼š[Data Engineering Zoomcamp 2025 with Kestra - YouTube](https://www.youtube.com/playlist?list=PLEK3H8YwZn1oPPShk2p5k3E9vO-gPnUCf)

ä¸»è¦å†…å®¹ï¼šä½¿ç”¨kestraæ·±å…¥ç ”ç©¶å·¥ä½œæµç¼–æŽ’ã€‚

Kestraï¼šä¸€ä¸ªå¼€æºçš„ï¼Œç”±äº‹ä»¶é©±åŠ¨çš„ç¼–æŽ’å¹³å°ã€‚ç”¨YAMLæž„å»ºå·¥ä½œæµã€‚

---

# Course Structure

## 1. æ¦‚å¿µï¼šç¼–æŽ’å’ŒKestra

å­¦ä¹ å·¥ä½œæµç¼–æŽ’çš„åŸºç¡€ï¼Œå…¶é‡è¦æ€§ä»¥åŠKestraå¦‚ä½•å»ºç«‹ç¼–æŽ’çŽ¯å¢ƒã€‚

### Videos

- **2.2.1 - Introduction to Workflow Orchestration**  
  [![2.2.1 - Workflow Orchestration Introduction](https://markdown-videos-api.jorgenkh.no/url?url=https%3A%2F%2Fyoutu.be%2FNp6QmmcgLCs)](https://youtu.be/Np6QmmcgLCs)

- **2.2.2 - Learn the Concepts of Kestra**  
  [![Learn Kestra](https://markdown-videos-api.jorgenkh.no/url?url=https%3A%2F%2Fyoutu.be%2Fo79n-EVpics)](https://youtu.be/o79n-EVpics)

### Resources

- [Quickstart Guide](https://go.kestra.io/de-zoomcamp/quickstart)
- [Install Kestra with Docker Compose](https://go.kestra.io/de-zoomcamp/docker-compose)
- [Tutorial](https://go.kestra.io/de-zoomcamp/tutorial)
- [What is an Orchestrator?](https://go.kestra.io/de-zoomcamp/what-is-an-orchestrator)

### Quickstart

ï¼ˆ1ï¼‰é€šè¿‡dockerå¯åŠ¨kestraå®¹å™¨

```
docker run --pull=always --rm -it -p 8080:8080 --user=root \
-v /var/run/docker.sock:/var/docker.sock  \
-v /tmp:/tmp kestra/kestra:latest-full server local
```

åœ¨æµè§ˆå™¨ä¸­è¾“å…¥localhost:8080 è¿›å…¥ç½‘é¡µç‰ˆã€‚

ï¼ˆ2ï¼‰åŸºæœ¬æ¦‚å¿µ

workflowsçš„ä¸‰å±žæ€§

- id

- namespace -å‘½åç©ºé—´

- tasks

Inputs è¾“å…¥

Outputs è¾“å‡º

Triggers è§¦å‘å™¨

---

## 2.  åŠ¨æ‰‹ç¼–ç é¡¹ç›®ï¼šä½¿ç”¨Kestraæž„å»ºæ•°æ®ç®¡é“

> Video Source: [DE Zoomcamp 2.2.3 - ETL Pipelines with Postgres in Kestra - YouTube](https://www.youtube.com/watch?v=OkfLX28Ecjg&list=PLEK3H8YwZn1oPPShk2p5k3E9vO-gPnUCf&index=4)
> 
> CSV files:  [Releases Â· DataTalksClub/nyc-tlc-data Â· GitHub](https://github.com/DataTalksClub/nyc-tlc-data/releases)

å¼€å¯ä¸€ä¸ªpostgreçš„å®¹å™¨

```
docker run -it  
-e POSTGRES_USER="kestra"  
-e POSTGRES_PASSWORD="k3str4"  
-e POSTGRES_DB="kestra"  
-v ./taxi_postgress_data:/var/lib/postgressql/data  
-p 5432:5432  
postgres
```

ä½¿ç”¨pgAdminè¿žæŽ¥è¯¥æ•°æ®åº“ï¼š

![](/Users/sitara/Library/Application%20Support/marktext/images/2025-02-02-20-30-53-image.png)

### 1: Flow explanation step by step

![](/Users/sitara/coding/zoomcamp/data-engineering-zoomcamp-main/02-workflow-orchestration/myhomework/local1.jpg)

#### Variables

```
variables:
  file: "{{inputs.taxi}}_tripdata_{{inputs.year}}-{{inputs.month}}.csv"
  staging_table: "public.{{inputs.taxi}}_tripdata_staging"
  table: "public.{{inputs.taxi}}_tripdata"
  data: "{{outputs.extract.outputFiles[inputs.taxi ~ '_tripdata_' ~ inputs.year ~ '-' ~ inputs.month ~ '.csv']}}""
```

fileï¼šåŸºäºŽinputsä¸­é€‰æ‹©çš„å‡ºç§Ÿè½¦ç±»åž‹ã€å¹´å’Œæœˆï¼Œç”ŸæˆCSVæ–‡ä»¶çš„åç§°ï¼Œæ ¼å¼å°†ä¼šä¸ºâ€œyellow_tripdata_2019-01.csvâ€

staging_table: å¯¹ç»™å®štaxiã€yearå’Œmonthæ”¾å…¥ä¸´æ—¶è¡¨ä¸­ã€‚

table: æŒ‡å®šPostgreSQLä¸­çš„æœ€ç»ˆè¡¨åç§°ï¼Œåˆå¹¶çš„æ•°æ®å°†ä¼šå­˜å…¥ç‰¹å®šå‡ºç§Ÿè½¦ç±»åž‹çš„è¡¨ä¸­ï¼Œè¡¨åä¾‹å¦‚ï¼špublic.green_tripdataã€‚

data: æŒ‡æå–ä»»åŠ¡ç”Ÿæˆçš„è¾“å‡ºæ–‡ä»¶ï¼Œç”¨ä½œå¤åˆ¶ä»»åŠ¡çš„è¾“å…¥ã€‚

#### Task:Set Lables

å°†æ ‡ç­¾æ·»åŠ åˆ°æµç¨‹æ‰§è¡Œä¸­ï¼Œä»¥è·Ÿè¸ªæ‰€é€‰æ–‡ä»¶å’Œå‡ºç§Ÿè½¦ç±»åž‹ã€‚æ ‡ç­¾æ˜¯æœ‰åŠ©äºŽç»„ç»‡ï¼Œè¯†åˆ«å’Œè·Ÿè¸ªå·¥ä½œæµæ‰§è¡Œçš„å…ƒæ•°æ®æ ‡ç­¾ã€‚ä»–ä»¬å¯ä»¥åœ¨è¿è¡Œæ—¶æˆ–æŸ¥çœ‹æ—¥å¿—å¹¶ç›‘è§†å·¥ä½œæµæ‰§è¡Œæ—¶æä¾›æœ‰ä»·å€¼çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚

#### Task:Extract Data

```
  - id: extract
    type: io.kestra.plugin.scripts.shell.Commands
    outputFiles:
      - "*.csv"
    taskRunner:
      type: io.kestra.plugin.core.runner.Process
    commands:
      - wget -qO- https://github.com/DataTalksClub/nyc-tlc-data/releases/download/{{inputs.taxi}}/{{render(vars.file)}}.gz | gunzip > {{render(vars.file)}}
```

ä½¿ç”¨GITHUBå­˜å‚¨åº“ä¸­çš„WGETOTORITIONAä¸‹è½½åŽ‹ç¼©çš„CSVæ–‡ä»¶ï¼Œå¹¶å°†å…¶è§£åŽ‹ç¼©å¹¶å°†å…¶ä¿å­˜ä¸º.csvæ–‡ä»¶ã€‚

#### Task:yellow_table/green_table

```
- id: yellow_table
    runIf: "{{inputs.taxi == 'yellow'}}"
    type: io.kestra.plugin.jdbc.postgresql.Queries
    sql: |
      CREATE TABLE IF NOT EXISTS {{render(vars.table)}} (
          unique_row_id          text,
          filename                text,
          VendorID               text,

        ...
      );
```

{{render(vars.table)}}ï¼Œéœ€è¦åˆ©ç”¨renderå‡½æ•°æ¥æ¸²æŸ“å…·æœ‰è¡¨è¾¾å¼çš„å˜é‡ã€‚

è¯¥è¡¨ä¸­å¢žåŠ ä¸¤åˆ—ï¼šå”¯ä¸€çš„IDåˆ—å’Œæ–‡ä»¶åã€‚å› æ­¤æˆ‘ä»¬å¯ä»¥æŸ¥çœ‹æ•°æ®åˆ°å“ªä¸ªæ–‡ä»¶ä»¥åŠåŸºäºŽæ•°æ®ç”Ÿæˆçš„å”¯ä¸€IDï¼Œä»¥é˜²æ­¢ä»¥åŽæ·»åŠ é‡å¤é¡¹ã€‚

#### Task: yellow_create_staging_table/green_create_staging_table

```
  - id: yellow_create_staging_table
    runIf: "{{inputs.taxi == 'yellow'}}"
    type: io.kestra.plugin.jdbc.postgresql.Queries
    sql: |
      CREATE TABLE IF NOT EXISTS {{render(vars.staging_table)}} (
          VendorID               text,
          tpep_pickup_datetime   timestamp,
          tpep_dropoff_datetime  timestamp,
          passenger_count        integer,

          ...
      );
```

ä¸ºæ¯æœˆçš„é»„è‰²/ç»¿è‰²å‡ºç§Ÿè½¦æ•°æ®åˆ›å»ºä¸€ä¸ªä¸´æ—¶è¡¨ï¼Œå¹¶ä¸ŽCSVæ–‡ä»¶å¯¹é½ã€‚

#### Task:  truncate_table

åŠ è½½æ–°æ•°æ®ä¹‹å‰ï¼Œæ¸…ç©ºä¸´æ—¶è¡¨ã€‚

#### Task: green_copy_in_to_staging_table/yellow_copy_in_to_staging_table

```
  - id: green_copy_in_to_staging_table
    runIf: "{{inputs.taxi == 'green'}}"
    type: io.kestra.plugin.jdbc.postgresql.CopyIn
    format: CSV
    from: "{{render(vars.data)}}"
    table: "{{render(vars.staging_table)}}"
    header: true
    columns: [VendorID,lpep_pickup_datetime, ...]
```

æ­¤ä»»åŠ¡è´Ÿè´£å°†æå–çš„CSVæ–‡ä»¶ä¸­çš„æ•°æ®å¤åˆ¶åˆ°ç”¨äºŽå¤„ç†çš„ä¸´æ—¶PostgreSQLè¡¨ä¸­ã€‚

#### Task: yellow_add_unique_id_and_filename/green_add_unique_id_and_filename

```
  - id: yellow_add_unique_id_and_filename
    runIf: "{{inputs.taxi == 'yellow'}}"
    type: io.kestra.plugin.jdbc.postgresql.Queries
    sql: |
      ALTER TABLE {{render(vars.staging_table)}}
      ADD COLUMN IF NOT EXISTS unique_row_id text,
      ADD COLUMN IF NOT EXISTS filename text;

      UPDATE {{render(vars.staging_table)}}
      SET 
        unique_row_id = md5(
          COALESCE(CAST(VendorID AS text), '') ||
          COALESCE(CAST(tpep_pickup_datetime AS text), '') || 
          COALESCE(CAST(tpep_dropoff_datetime AS text), '') || 
          COALESCE(PULocationID, '') || 
          COALESCE(DOLocationID, '') || 
          COALESCE(CAST(fare_amount AS text), '') || 
          COALESCE(CAST(trip_distance AS text), '')      
        ),
        filename = '{{render(vars.file)}}';
```

é€šè¿‡ä¸ºæ¯ä¸€è¡Œç”Ÿæˆå”¯ä¸€çš„å“ˆå¸ŒIDå¹¶å­˜å‚¨æ–‡ä»¶åæ¥æ›´æ–°è¡¨ã€‚

#### Task: yellow_merge_data/green_merge_data

```
  - id: yellow_merge_data
    runIf: "{{inputs.taxi == 'yellow'}}"
    type: io.kestra.plugin.jdbc.postgresql.Queries
    sql: |
      MERGE INTO {{render(vars.table)}} AS T
      USING {{render(vars.staging_table)}} AS S
      ON T.unique_row_id = S.unique_row_id
      WHEN NOT MATCHED THEN
        INSERT (
          unique_row_id, filename, VendorID, tpep_pickup_datetime, tpep_dropoff_datetime,
          passenger_count, trip_distance, RatecodeID, store_and_fwd_flag, PULocationID,
          DOLocationID, payment_type, fare_amount, extra, mta_tax, tip_amount, tolls_amount,
          improvement_surcharge, total_amount, congestion_surcharge
        )
        VALUES (
          S.unique_row_id, S.filename, S.VendorID, S.tpep_pickup_datetime, S.tpep_dropoff_datetime,
          S.passenger_count, S.trip_distance, S.RatecodeID, S.store_and_fwd_flag, S.PULocationID,
          S.DOLocationID, S.payment_type, S.fare_amount, S.extra, S.mta_tax, S.tip_amount, S.tolls_amount,
          S.improvement_surcharge, S.total_amount, S.congestion_surcharge
        );
```

ä½¿ç”¨unique_row_idå°†æ¯æœˆæ•°æ®ä»Žä¸´æ—¶è¡¨åˆå¹¶åˆ°yellow_table/green_tableä¸­

- type: io.kestra.plugin.jdbc.postgresql.cerqueriesï¼šåœ¨PostgreSQLæ•°æ®åº“ä¸Šæ‰§è¡ŒSQLæŸ¥è¯¢ã€‚

```
MERGE INTO {{render(vars.table)}} AS T
USING {{render(vars.staging_table)}} AS S
ON T.unique_row_id = S.unique_row_id
```

- åˆå¹¶ä¸ºtï¼šå°†æ¯æœˆè¡¨ï¼ˆsï¼‰çš„æ•°æ®åˆå¹¶åˆ°æœ€ç»ˆè¡¨ï¼ˆtï¼‰ä¸­ã€‚åœ¨t.unique_row_idä¸Š= s.unique_row_idï¼šåŸºäºŽunique_row_idåˆ—ä¸Žæºï¼ˆsï¼‰å’Œtargetï¼ˆtï¼‰çš„è¡ŒåŒ¹é…ã€‚å¦‚æžœåœ¨tä¸­å­˜åœ¨å…·æœ‰ç›¸åŒå”¯ä¸€çš„_row_idçš„è®°å½•ï¼Œåˆ™å°†å¿½ç•¥å®ƒã€‚

- WHEN NOT MATCHED THEN: Ensures that only records that do not already exist in T are inserted.  å½“ä¸åŒ¹é…æ—¶ï¼šç¡®ä¿ä»…æ’å…¥tä¸­å°šæœªå­˜åœ¨çš„è®°å½•ã€‚INSERT VALUES: Inserts all relevant columns from the monthly table,  
  æ’å…¥å€¼ï¼šä»Žæ¯æœˆè¡¨ä¸­æ’å…¥æ‰€æœ‰ç›¸å…³åˆ—ï¼Œ

#### Task: purge_files

æ­¤ä»»åŠ¡å¯ç¡®ä¿åœ¨æµé‡æ‰§è¡Œè¿‡ç¨‹ä¸­ä¸‹è½½æˆ–ç”Ÿæˆçš„ä»»ä½•æ–‡ä»¶ä¸€æ—¦ä¸å†éœ€è¦ï¼Œå°±å¯ä»¥åˆ é™¤ã€‚

#### Plugin Defaults

æ‰€æœ‰PostgreSQLä»»åŠ¡éƒ½ä½¿ç”¨é¢„é…ç½®çš„è¿žæŽ¥ï¼š

URL: jdbc:postgresql://host.docker.internal:5433/ny_taxi Username: kestra Password:k3str4

### 2: Execute flow

Try the flow 02_postgres_taxi.

![](/Users/sitara/Library/Application%20Support/marktext/images/2025-02-02-21-32-01-image.png)

### 3:check PgAdmin



## Load Data to Local Postgres with backfill

> video source:[[DE Zoomcamp 2.2.4 - Manage Scheduling and Backfills with Postgres in Kestra - YouTube](https://www.youtube.com/watch?v=_-li_z97zog)]()

> The flow code: 



### 1:Flow explanation

**concurrency**

æ•´ä¸ªflowä¸­åªæœ‰ä¸€å¼ ä¸´æ—¶è¡¨ï¼Œæ™ºèƒ½æ¯æ¬¡åªæ‰§è¡Œä¸€ä¸ªæ“ä½œï¼Œé˜²æ­¢å¤šä¸ªè¿›ç¨‹å†™ä¸åŒæœˆä»½åˆ°åŒä¸€ä¸ªè¡¨ä¸­ã€‚

**Triggers: green_schedule**

Cronè¡¨è¾¾å¼ï¼šâ€œ 0 9 1 * *â€ï¼šæ¯ä¸ªæœˆçš„ç¬¬ä¸€å¤©åœ¨ä¸Šåˆ9:00è¿è¡Œã€‚

**Triggers: yellow_schedule**

Cronè¡¨è¾¾å¼ï¼šâ€œ 0 10 1 * *â€ï¼šæ¯æœˆçš„ç¬¬ä¸€å¤©åœ¨ä¸Šåˆ10:00è¿è¡Œã€‚

### 2:Execute flow

Select triggers --> Backfill executions

![](/Users/sitara/Library/Application%20Support/marktext/images/2025-02-02-21-51-58-image.png)

### 3: Check PgAdmin

---

## ETL Pipelines in Kestra: Google Cloud Platform

Now that you've learned how to build ETL pipelines locally using Postgres, we are ready to move to the cloud. In this section, we'll load the same Yellow and Green Taxi data to Google Cloud Platform (GCP) using: 

1. Google Cloud Storage (GCS) as a data lake  
2. BigQuery as a data warehouse.

### Videos

- **2.2.6 - Create an ETL Pipeline with GCS and BigQuery in Kestra**  
  [![Create an ETL Pipeline with BigQuery in Kestra](https://markdown-videos-api.jorgenkh.no/url?url=https%3A%2F%2Fyoutu.be%2FnKqjjLJ7YXs)](https://youtu.be/nKqjjLJ7YXs)
- **2.2.7 - Manage Scheduling and Backfills using BigQuery in Kestra**   
  [![Manage Scheduling and Backfills using BigQuery in Kestra](https://markdown-videos-api.jorgenkh.no/url?url=https%3A%2F%2Fyoutu.be%2FDoaZ5JWEkH0)](https://youtu.be/DoaZ5JWEkH0)
- **2.2.8 - Transform Data with dbt and BigQuery in Kestra**   
  [![Transform Data with dbt and BigQuery in Kestra](https://markdown-videos-api.jorgenkh.no/url?url=https%3A%2F%2Fyoutu.be%2FeF_EdV4A1Wk)](https://youtu.be/eF_EdV4A1Wk)

### Setup Google Cloud Platform (GCP)

Before we start loading data to GCP, we need to set up the Google Cloud Platform. 

First, adjust the following flow [`04_gcp_kv.yaml`](flows/04_gcp_kv.yaml) to include your service account, GCP project ID, BigQuery dataset and GCS bucket name (_along with their location_) as KV Store values:

- GCP_CREDS
- GCP_PROJECT_ID
- GCP_LOCATION
- GCP_BUCKET_NAME
- GCP_DATASET.

> [!WARNING]  
> The `GCP_CREDS` service account contains sensitive information. Ensure you keep it secure and do not commit it to Git. Keep it as secure as your passwords.

### Create GCP Resources

If you haven't already created the GCS bucket and BigQuery dataset in the first week of the course, you can use this flow to create them: [`05_gcp_setup.yaml`](flows/05_gcp_setup.yaml).

### GCP Workflow: Load Taxi Data to BigQuery

```mermaid
graph LR
  SetLabel[Set Labels] --> Extract[Extract CSV Data]
  Extract --> UploadToGCS[Upload Data to GCS]
  UploadToGCS -->|Taxi=Yellow| BQYellowTripdata[Main Yellow Tripdata Table]:::yellow
  UploadToGCS -->|Taxi=Green| BQGreenTripdata[Main Green Tripdata Table]:::green
  BQYellowTripdata --> BQYellowTableExt[External Table]:::yellow
  BQGreenTripdata --> BQGreenTableExt[External Table]:::green
  BQYellowTableExt --> BQYellowTableTmp[Monthly Table]:::yellow
  BQGreenTableExt --> BQGreenTableTmp[Monthly Table]:::green
  BQYellowTableTmp --> BQYellowMerge[Merge to Main Table]:::yellow
  BQGreenTableTmp --> BQGreenMerge[Merge to Main Table]:::green
  BQYellowMerge --> PurgeFiles[Purge Files]
  BQGreenMerge --> PurgeFiles[Purge Files]

  classDef yellow fill:#FFD700,stroke:#000,stroke-width:1px;
  classDef green fill:#32CD32,stroke:#000,stroke-width:1px;
```

The flow code: [`06_gcp_taxi.yaml`](flows/06_gcp_taxi.yaml).

### GCP Workflow: Schedule and Backfill Full Dataset

We can now schedule the same pipeline shown above to run daily at 9 AM UTC for the green dataset and at 10 AM UTC for the yellow dataset. You can backfill historical data directly from the Kestra UI.

Since we now process data in a cloud environment with infinitely scalable storage and compute, we can backfill the entire dataset for both the yellow and green taxi data without the risk of running out of resources on our local machine.

The flow code: [`06_gcp_taxi_scheduled.yaml`](flows/06_gcp_taxi_scheduled.yaml).

### GCP Workflow: Orchestrate dbt Models

Now that we have raw data ingested into BigQuery, we can use dbt to transform that data. The flow will sync the dbt models from Git to Kestra and run the `dbt build` command to build the models:

```mermaid
graph LR
  Start[Select dbt command] --> Sync[Sync Namespace Files]
  Sync --> Build[Run dbt Build Command]
```

The flow code: [`07_gcp_dbt.yaml`](flows/07_gcp_dbt.yaml).

---

## Bonus: Deploy to the Cloud

Now that we've got our ETL pipeline working both locally and in the cloud, we can deploy Kestra to the cloud so it can continue to orchestrate our ETL pipelines monthly with our configured schedules, We'll cover how you can install Kestra on Google Cloud in Production, and automatically sync and deploy your workflows from a Git repository.

### Videos

- **2.2.9 - Deploy Workflows to the Cloud with Git**   
  [![Deploy Workflows to the Cloud with Git](https://markdown-videos-api.jorgenkh.no/url?url=https%3A%2F%2Fyoutu.be%2Fl-wC71tI3co)](https://youtu.be/l-wC71tI3co)

Resources

- [Install Kestra on Google Cloud](https://go.kestra.io/de-zoomcamp/gcp-install)
- [Moving from Development to Production](https://go.kestra.io/de-zoomcamp/dev-to-prod)
- [Using Git in Kestra](https://go.kestra.io/de-zoomcamp/git)
- [Deploy Flows with GitHub Actions](https://go.kestra.io/de-zoomcamp/deploy-github-actions)

## Additional Resources ðŸ“š

- Check [Kestra Docs](https://go.kestra.io/de-zoomcamp/docs)
- Explore our [Blueprints](https://go.kestra.io/de-zoomcamp/blueprints) library
- Browse over 600 [plugins](https://go.kestra.io/de-zoomcamp/plugins) available in Kestra
- Give us a star on [GitHub](https://go.kestra.io/de-zoomcamp/github)
- Join our [Slack community](https://go.kestra.io/de-zoomcamp/slack) if you have any questions
- Find all the videos in this [YouTube Playlist](https://go.kestra.io/de-zoomcamp/yt-playlist)

# 

---

# Community notes

Did you take notes? You can share them by creating a PR to this file! 

* [Notes from Manuel Guerra)](https://github.com/ManuelGuerra1987/data-engineering-zoomcamp-notes/blob/main/2_Workflow-Orchestration-(Kestra)/README.md)
* [Notes from Horeb Seidou](https://spotted-hardhat-eea.notion.site/Week-2-Workflow-Orchestration-17129780dc4a80148debf61e6453fffe)
* Add your notes above this line

---

# Previous Cohorts

* 2022: [notes](../../2022/week_2_data_ingestion#community-notes) and [videos](../../2022/week_2_data_ingestion/)
* 2023: [notes](../../2023/week_2_workflow_orchestration#community-notes) and [videos](../../2023/week_2_workflow_orchestration/)
* 2024: [notes](../../2024/02-workflow-orchestration#community-notes) and [videos](../../2024/02-workflow-orchestration/)
